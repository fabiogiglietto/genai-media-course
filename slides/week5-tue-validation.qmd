---
title: "Validazione: Umano vs Macchina"
subtitle: "IA Generativa e Media â€” Settimana 5"
author: "Fabio Giglietto"
institute: "DISCUI Â· UniversitÃ  degli Studi di Urbino Carlo Bo"
date: "24 Marzo 2026"
date-format: "D MMMM YYYY"
format:
  revealjs:
    theme: [default, ../_extensions/uniurb/discui.scss]
    logo: ../assets/logo-uniurb-white.svg
    footer: "IA Generativa e Media Â· A.A. 2025/2026"
    slide-number: c/t
    transition: fade
    width: 1920
    height: 1080
    margin: 0.08
    center: false
    hash: true
    controls: true
    progress: true
execute:
  echo: true
  warning: false
  message: false
  fig-width: 10
  fig-height: 6
  fig-dpi: 150
knitr:
  opts_chunk:
    dev: "ragg_png"
lang: it
bibliography: ../references.bib
csl: ../apa.csl
---

## Roadmap della sessione

1. **Perche' validare** â€” il ruolo della verifica umana
2. **La metodologia LLMs-in-the-loop** â€” pipeline e principi
3. **Progettare il protocollo di validazione** â€” campionamento e codifica
4. **Misurare l'accordo** â€” Cohen's kappa e Krippendorff's alpha
5. **Esercizio pratico** â€” validare un campione di commenti
6. **Iterazione e raffinamento** â€” migliorare la pipeline

::: {.notes}
Struttura della sessione. Oggi e' il giorno chiave per la qualita' metodologica del progetto. Ieri i gruppi hanno classificato i commenti con Gemini; oggi confrontiamo quelle classificazioni con il giudizio umano. L'obiettivo e' duplice: (1) verificare quanto l'IA e' affidabile e (2) apprendere la metodologia di validazione che sara' documentata nel paper. Questa sessione alterna teoria (30%) e pratica (70%).
:::

## Perche' Validare {background-color="#C5612E"}

::: {.notes}
Sezione 1: motivazione della validazione. Perche' non basta la classificazione automatizzata.
:::

## L'IA non e' un oracolo

::: {.highlight-box}
**Principio fondamentale:** come ci ricorda @cosenza2025, l'IA ragiona per **probabilita'**, non per verita'. Ogni classificazione automatizzata e' una **bozza** che richiede verifica.
:::

Perche' la classificazione dell'IA puo' essere inaffidabile:

- **Sensibilita' al prompt:** piccole variazioni producono risultati diversi
- **Bias del modello:** tendenza a sovra-rappresentare alcune categorie
- **Ambiguita' linguistica:** ironia, sarcasmo, dialetto sono difficili da interpretare
- **Mancanza di contesto:** l'IA non conosce il contesto culturale specifico

::: {.notes}
Collegamento con la regola 4 di Cosenza (2025): "Valuta". L'IA non e' stata progettata per dire la verita', ma per generare l'output piu' probabile. Nel contesto della classificazione dei commenti, questo significa che l'IA puo' classificare correttamente i casi chiari ma sbagliare sistematicamente sui casi ambigui. L'ironia e' particolarmente problematica: un commento come "bellissimo, proprio come una foto vera!" potrebbe essere sarcasmo (riconoscimento IA) o genuino (engagement). Il contesto culturale italiano aggiunge complessita': espressioni dialettali, riferimenti locali, humor specifico. Per questo la validazione umana non e' un optional ma un requisito metodologico.
:::

## Il paradosso della fiducia nell'IA

::: {.columns}
::: {.column width="50%"}
### Il rischio

Ferrara (2026) descrive un **paradosso**: piu' l'IA sembra affidabile, piu' rischiamo di fidarci ciecamente dei suoi output [@ferrara2026].

Nella ricerca, questo si traduce in:

- Accettare classificazioni senza verifica
- Non documentare gli errori dell'IA
- Presentare risultati non validati
:::

::: {.column width="50%"}
### La soluzione

La **validazione sistematica** protegge da questo rischio:

- Codifica umana indipendente
- Confronto quantitativo IA-umano
- Documentazione trasparente
- Iterazione basata sugli errori
:::
:::

::: {.notes}
Il collegamento con Ferrara (2026) e' importante: il Generative AI Paradox si applica anche alla ricerca. Come i cittadini possono cadere nella "credulity" (credere a contenuti sintetici convincenti), i ricercatori possono cadere nell'accettare acriticamente classificazioni IA apparentemente plausibili. La validazione sistematica e' l'antidoto: trattare l'IA come un codificatore le cui classificazioni devono essere verificate, esattamente come faremmo con un codificatore umano alle prime armi. Questo approccio e' al centro della metodologia LLMs-in-the-loop.
:::

## La Metodologia LLMs-in-the-loop {background-color="#C5612E"}

::: {.notes}
Sezione 2: approfondimento della metodologia. Qui espandiamo quanto introdotto nel lancio del progetto.
:::

## La pipeline LLMs-in-the-loop

::: {.orange-box}
### Definizione
La metodologia *LLMs-in-the-loop* integra l'analisi automatizzata dei LLM con la validazione sistematica umana, trattando l'IA come un "codificatore" le cui classificazioni vengono verificate contro il giudizio umano [@marino2024].
:::

La pipeline prevede un **ciclo iterativo:**

1. Definizione del codebook
2. Prompting sistematico al LLM
3. Classificazione automatizzata
4. **Validazione umana** (focus di oggi)
5. Calcolo dell'affidabilita' inter-codificatore
6. Iterazione e raffinamento

::: {.notes}
Ripresa piu' approfondita della metodologia LLMs-in-the-loop di Marino & Giglietto (2024). Il punto chiave e' il ciclo iterativo: non si tratta di far classificare all'IA e poi verificare una volta. Si tratta di un processo in cui la validazione umana alimenta il raffinamento del prompt e del codebook, migliorando progressivamente la qualita' della classificazione. Oggi ci concentriamo sulle fasi 4 e 5; nella parte finale della sessione affronteremo la fase 6. Il seminario di Bruna Paroni nella settimana 3 ha illustrato questa metodologia con un caso studio brasiliano.
:::

## L'IA come "codificatore"

Nella ricerca tradizionale sull'analisi del contenuto, si usano **codificatori umani** multipli per garantire affidabilita'.

Con la pipeline LLMs-in-the-loop, l'IA diventa un **codificatore aggiuntivo**:

| Aspetto | Codificatore umano | Codificatore IA |
|---------|-------------------|-----------------|
| **Scala** | Centinaia di unita' | Migliaia di unita' |
| **Coerenza** | Fatica, distrazione | Dipende dal prompt |
| **Contesto** | Comprensione profonda | Comprensione superficiale |
| **Costo** | Alto (tempo, risorse) | Basso (computazionale) |
| **Verifica** | Intercoder reliability | IA vs. umano reliability |

::: {.notes}
Tabella comparativa che illustra i punti di forza e debolezza di codificatori umani e IA. L'IA eccelle nella scala (puo' classificare migliaia di commenti in pochi minuti) e nel costo (quasi nullo). L'umano eccelle nella comprensione del contesto e nella capacita' di cogliere sfumature. L'approccio LLMs-in-the-loop sfrutta entrambi: l'IA per la scala, l'umano per la qualita' e la validazione. La metrica di affidabilita' standard (intercoder reliability) viene applicata al confronto IA-umano esattamente come si applicherebbe al confronto tra due codificatori umani.
:::

## Progettare il Protocollo di Validazione {background-color="#C5612E"}

::: {.notes}
Sezione 3: aspetti pratici del protocollo di validazione. Come selezionare il campione e organizzare la codifica umana.
:::

## Selezionare il campione

::: {.highlight-box}
**Regola pratica:** validare almeno il **20-30%** del dataset totale, con un minimo di **50 unita'** per l'analisi di affidabilita'.
:::

Strategie di campionamento:

- **Campione casuale:** selezione randomizzata dal dataset completo
- **Campione stratificato:** includere commenti da tutte le categorie IA
- **Campione mirato:** includere i casi classificati come "incerti" dall'IA

::: {.callout-tip}
## Suggerimento
Combinate le strategie: un campione casuale (per la rappresentativita') + tutti i casi "incerti" dell'IA (per capire dove l'IA fatica di piu').
:::

::: {.notes}
Il campionamento e' cruciale per la validita' della validazione. Il 20-30% e' una regola pratica consolidata nell'analisi del contenuto. Con dataset di 200-300 commenti (tipico per i gruppi), questo significa 50-80 commenti da classificare manualmente. Il campione stratificato garantisce che tutte le categorie siano rappresentate â€” importante perche' alcune categorie (es. "riconoscimento IA") potrebbero essere rare. I casi "incerti" sono particolarmente informativi: mostrano dove il codebook e' ambiguo e dove il prompt va migliorato.
:::

## Organizzare la codifica umana

**Preparazione:**

1. Estrarre il campione in un **foglio separato**
2. Rimuovere la colonna con la **classificazione IA** (codifica cieca)
3. Distribuire il campione ai **codificatori umani**

**Codifica:**

4. Ogni codificatore classifica **indipendentemente**
5. Usare lo **stesso codebook** usato per il prompt IA
6. Annotare i casi **difficili** e le motivazioni

**Confronto:**

7. Riunire le classificazioni umane e confrontare con quelle IA
8. Calcolare le metriche di **accordo**

::: {.notes}
Il punto chiave e' la codifica cieca: i codificatori umani non devono vedere la classificazione dell'IA per evitare il bias di ancoraggio (tendenza a conformarsi alla classificazione gia' presente). Idealmente, almeno 2 membri del gruppo codificano indipendentemente lo stesso campione: questo permette di calcolare sia l'accordo umano-umano (baseline) che l'accordo IA-umano. Se il gruppo ha 5 membri, suggerire 2 codificatori sul campione completo e gli altri 3 su sottoinsiemi. I casi difficili e le motivazioni alimentano la fase di iterazione.
:::

## Misurare l'Accordo {background-color="#C5612E"}

::: {.notes}
Sezione 4: le metriche statistiche per misurare l'accordo tra codificatori. Concetti tecnici necessari per il paper.
:::

## Perche' non basta la percentuale di accordo

::: {.columns}
::: {.column width="50%"}
### Il problema

Due codificatori possono concordare **per caso**.

Se ci sono 2 categorie equiprobabili, il 50% di accordo e' atteso anche con classificazioni casuali.

La **percentuale di accordo grezzo** non tiene conto del caso.
:::

::: {.column width="50%"}
### La soluzione

Le metriche corrette sottraggono l'**accordo atteso per caso**:

$$\kappa = \frac{P_o - P_e}{1 - P_e}$$

Dove:

- $P_o$ = accordo osservato
- $P_e$ = accordo atteso per caso
:::
:::

::: {.notes}
Concetto statistico fondamentale per la validazione. La percentuale di accordo grezzo (quante volte i codificatori concordano sul totale) e' una metrica ingannevole perche' non considera l'accordo casuale. Se abbiamo 5 categorie, l'accordo casuale atteso e' circa il 20%. Un accordo del 60% sembra buono ma dopo la correzione potrebbe essere modesto. Le metriche kappa e alpha correggono questo problema sottraendo l'accordo atteso per caso dal denominatore.
:::

## Cohen's kappa e Krippendorff's alpha

| Metrica | Uso | Caratteristiche |
|---------|-----|-----------------|
| **Cohen's kappa** ($\kappa$) | 2 codificatori | Semplice, ampiamente usato |
| **Krippendorff's alpha** ($\alpha$) | 2+ codificatori | Flessibile, gestisce dati mancanti |

::: {.highlight-box}
**Scale interpretative standard:**

| Valore | Interpretazione |
|--------|----------------|
| < 0.20 | Accordo scarso |
| 0.21 - 0.40 | Accordo discreto |
| 0.41 - 0.60 | Accordo moderato |
| 0.61 - 0.80 | Accordo buono |
| 0.81 - 1.00 | Accordo eccellente |
:::

::: {.notes}
Le due metriche principali per la validazione. Cohen's kappa e' la piu' usata quando si confrontano 2 codificatori (nel nostro caso, IA vs. umano). Krippendorff's alpha e' piu' flessibile: funziona con piu' di 2 codificatori e gestisce i dati mancanti. Per il progetto, il kappa di Cohen e' sufficiente se si confronta 1 codificatore umano con l'IA; alpha di Krippendorff se si confrontano 2 codificatori umani + l'IA. L'obiettivo minimo per la ricerca e' un kappa/alpha di 0.60 (accordo buono). Valori inferiori indicano che il codebook o il prompt vanno rivisti. Nella sezione Metodo del paper, bisogna riportare la metrica usata, il valore ottenuto e l'interpretazione.
:::

## Calcolare il kappa: la matrice di confusione

Esempio con 3 categorie e 50 commenti:

|  | **Umano: Riconosc.** | **Umano: Emotivo** | **Umano: Critico** | **Totale IA** |
|--|:---:|:---:|:---:|:---:|
| **IA: Riconosc.** | **12** | 2 | 1 | 15 |
| **IA: Emotivo** | 3 | **18** | 2 | 23 |
| **IA: Critico** | 0 | 1 | **11** | 12 |
| **Totale umano** | 15 | 21 | 14 | **50** |

$P_o = (12+18+11)/50 = 0.82$ --- Calcolo manuale o con Gemini [@cosenza2025].

::: {.notes}
Esempio concreto di matrice di confusione. I valori in grassetto sulla diagonale sono i casi di accordo. I valori fuori diagonale sono i disaccordi. L'accordo osservato e' 0.82 (82%). L'accordo atteso per caso si calcola dai totali marginali. Due approcci per il calcolo: (1) manuale con foglio di calcolo â€” creare la matrice, calcolare Po e Pe, applicare la formula; (2) con Gemini â€” caricare le due colonne e chiedere il calcolo. Come suggerisce Cosenza (2025), usate entrambi per verificare. La matrice rivela i pattern di errore: l'IA confonde "Riconoscimento" con "Emotivo" 3 volte, guidando il raffinamento del prompt.
:::

## Esercizio Pratico {background-color="#C5612E"}

::: {.notes}
Sezione 5: esercizio di validazione. La parte centrale della sessione.
:::

## Esercizio: validare un campione

::: {.orange-box}
### Attivita' (45 minuti)

**Fase 1 â€” Preparazione (10 min):**

- Selezionate un campione di **50 commenti** dal vostro dataset
- Create un foglio separato **senza** la classificazione IA
- 2 membri del gruppo codificano indipendentemente

**Fase 2 â€” Codifica (20 min):**

- Ogni codificatore classifica i 50 commenti usando il codebook
- Annotate i casi difficili

**Fase 3 â€” Confronto (15 min):**

- Confrontate le classificazioni umane con quelle IA
- Costruite la matrice di confusione
- Calcolate il kappa (manualmente o con Gemini)
:::

::: {.notes}
Esercizio pratico principale della sessione. Fase 1: la preparazione deve essere rapida; i dati sono gia' nel foglio di calcolo dal lab di ieri. Creare un foglio separato con solo ID e testo del commento, senza la colonna della classificazione IA. Fase 2: la codifica individuale deve essere davvero indipendente â€” i codificatori non devono comunicare durante questa fase. Fase 3: il confronto e' il momento piu' interessante â€” dove emergono i disaccordi e si capisce dove il codebook e' ambiguo. Passare tra i gruppi durante la Fase 3 per aiutare con il calcolo del kappa e l'interpretazione dei risultati.
:::

## Interpretare i risultati

::: {.columns}
::: {.column width="50%"}
### Se il kappa e' alto (> 0.60)

- La pipeline funziona bene
- L'IA e' un codificatore affidabile per queste categorie
- Si puo' procedere con la classificazione del dataset completo
- Documentare nel paper
:::

::: {.column width="50%"}
### Se il kappa e' basso (< 0.60)

- Analizzare la **matrice di confusione**: dove sbaglia l'IA?
- Rivedere il **codebook**: le categorie sono ambigue?
- Raffinare il **prompt**: servono piu' esempi?
- **Iterare:** ripetere la classificazione e la validazione
:::
:::

::: {.notes}
Guida all'interpretazione. Un kappa alto non significa che l'analisi e' finita â€” significa che possiamo fidarci della classificazione IA per il dataset completo. Un kappa basso non e' un fallimento â€” e' un'opportunita' di miglioramento. Le fonti di disaccordo piu' comuni: (1) categorie del codebook sovrapposte, (2) esempi insufficienti nel prompt, (3) commenti ambigui che richiedono contesto culturale. L'iterazione e' il cuore della metodologia: si raffina il prompt, si riclassifica, si ricalcola il kappa, fino a raggiungere un livello accettabile.
:::

## Iterazione e Raffinamento {background-color="#C5612E"}

::: {.notes}
Sezione 6: come migliorare la pipeline basandosi sui risultati della validazione.
:::

## Raffinamento e documentazione per il paper

::: {.columns}
::: {.column width="50%"}
### Ciclo di raffinamento

- Analizzare la **matrice di confusione**: dove sbaglia l'IA?
- Aggiungere **anti-esempi** al prompt (errori corretti)
- **Accorpare** categorie problematiche se necessario
- Ricalcolare il **kappa** dopo ogni iterazione
:::

::: {.column width="50%"}
### Cosa riportare nel paper

- **Codebook** con definizioni e esempi
- **Prompt finale** completo (in appendice)
- **Metriche**: kappa/alpha per categoria
- **Iterazioni**: cicli e miglioramenti
- **Modello IA**: versione e data
:::
:::

::: {.callout-note}
## Da ricordare
La **trasparenza metodologica** e' un valore fondamentale: documentate non solo i successi ma anche gli errori e le iterazioni. Questo rende la ricerca riproducibile e credibile.
:::

::: {.notes}
Due aspetti complementari: il raffinamento iterativo e la documentazione. Il ciclo prevede: analizzare errori, migliorare prompt e codebook, riclassificare, ricalcolare kappa. Gli anti-esempi nel prompt sono particolarmente efficaci: si aggiungono commenti che l'IA ha classificato erroneamente con la categoria corretta. Se dopo 2-3 iterazioni il kappa resta basso per alcune categorie, ripensare le categorie stesse. Per il paper, ogni elemento della pipeline deve essere riportato: prompt completo in appendice, metriche aggregate e per categoria, numero di iterazioni. Questo approccio segue i principi della scienza aperta e le raccomandazioni di Marino & Giglietto (2024).
:::

## Prossimi passi del progetto

| Data | Attivita' |
|------|----------|
| **Mer 25 Marzo** | Consultazione gruppi â€” revisione metodologica |
| **Lun 30 Marzo** | Workshop di scrittura â€” struttura del paper |
| **Mar 31 Marzo** | Lavoro di gruppo â€” stesura collaborativa |
| **Mer 1 Aprile** | Sintesi del corso â€” consultazioni finali |
| **2 sett. prima appello** | **Consegna paper finale** |

::: {.highlight-box}
**Per domani:** preparate una breve **presentazione** (5 minuti) dei risultati della validazione per la consultazione di gruppo. Includete: kappa ottenuto, problemi emersi, strategie di miglioramento.
:::

::: {.notes}
Timeline aggiornata del progetto. Domani e' la consultazione di gruppo: ogni gruppo presenta i risultati della validazione e discute i problemi metodologici. La settimana 6 e' dedicata alla scrittura. Il paper va consegnato 2 settimane prima dell'appello di giugno. Enfatizzare che la validazione non deve essere perfetta per domani â€” l'importante e' aver iniziato il processo e identificato i punti critici. La consultazione servira' per risolvere i problemi e pianificare la scrittura.
:::

## Grazie! {background-color="#C5612E"}

**Prossima lezione:** Consultazione Gruppi e Revisione (25 Marzo 2026)

ðŸ“§ fabio.giglietto@uniurb.it

ðŸŒ blended.uniurb.it

::: {.notes}
Chiusura. Ricordare: (1) completare la validazione se non terminata in aula, (2) preparare la presentazione per la consultazione di domani (5 minuti, informale), (3) portare i risultati del kappa e la matrice di confusione. Domani ogni gruppo avra' un momento dedicato di consultazione con il docente per discutere problemi specifici.
:::

## Riferimenti
